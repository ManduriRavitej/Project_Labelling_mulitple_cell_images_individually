{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport time\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport gc; gc.collect()\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.utils.data as D\nimport torch.nn.functional as F\n\nimport torchvision\nfrom torchvision import transforms as T\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_df_path=\"../input/hpa-cell-tiles-sample-balanced-dataset/cell_df.csv\"\ntrain_images_path=\"../input/hpa-cell-tiles-sample-balanced-dataset/cells\"","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_df=pd.read_csv(train_df_path)\ntrain_df = train_df.rename(columns = {'image_labels': 'Label'}, inplace = False)\ntrain_df.head()","metadata":{"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                               image_id    r_mean    g_mean    b_mean  \\\n0  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.019785  0.007022  0.081189   \n1  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.021645  0.011319  0.059531   \n2  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.026710  0.014573  0.054268   \n3  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.018123  0.009205  0.065854   \n4  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.029577  0.014019  0.037737   \n\n   cell_id Label  size1  size2  \n0        1     0    510    656  \n1        2     0    875    748  \n2        3     0    924    760  \n3        4     0    844    538  \n4        5     0    620   1168  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>r_mean</th>\n      <th>g_mean</th>\n      <th>b_mean</th>\n      <th>cell_id</th>\n      <th>Label</th>\n      <th>size1</th>\n      <th>size2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.019785</td>\n      <td>0.007022</td>\n      <td>0.081189</td>\n      <td>1</td>\n      <td>0</td>\n      <td>510</td>\n      <td>656</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.021645</td>\n      <td>0.011319</td>\n      <td>0.059531</td>\n      <td>2</td>\n      <td>0</td>\n      <td>875</td>\n      <td>748</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.026710</td>\n      <td>0.014573</td>\n      <td>0.054268</td>\n      <td>3</td>\n      <td>0</td>\n      <td>924</td>\n      <td>760</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.018123</td>\n      <td>0.009205</td>\n      <td>0.065854</td>\n      <td>4</td>\n      <td>0</td>\n      <td>844</td>\n      <td>538</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.029577</td>\n      <td>0.014019</td>\n      <td>0.037737</td>\n      <td>5</td>\n      <td>0</td>\n      <td>620</td>\n      <td>1168</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_df.Label.unique()","metadata":{"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"array(['0', '1|0', '5|1|13', '6|1', '1|2|0', '12|1|0', '1|2', '1',\n       '16|1|13', '1|0|13', '14|1|0', '12|1|2', '16|1|0', '9|1|13',\n       '16|1', '1|4', '16|1|2', '16|7|1|0', '12|5|1|0', '7|1|2|0', '10|1',\n       '9|1|0', '7|1', '14|1', '7|1|0|13', '1|3', '1|13', '5|1', '12|1',\n       '8|1|0', '1|2|13', '1|3|0|13', '8|1', '14|1|2|0', '1|3|0', '7|1|0',\n       '9|1|2|0', '16|5|1', '13|1', '5|1|0', '10|1|0', '16|1|4', '2', '3',\n       '4', '5', '6', '16|6', '6|14', '6|13', '16|6|0', '16|6|2', '6|2',\n       '6|0', '6|3', '12|6', '6|5', '16|6|13', '6|7', '16|6|3|0', '6|2|0',\n       '6|9', '6|5|0', '16|6|2|0', '6|17', '9|6|13', '6|10|2', '6|4',\n       '9|6', '7', '8', '9', '9|9', '9|13', '16|9|13', '16|9|2|0', '9|0',\n       '13|16|9|4|13', '13|16|9|13', '9|16|13', '9|14|0', '9|16',\n       '12|9|0', '16|9', '9|12|16', '9|5|0|17', '16|9|0', '9|4',\n       '12|16|9', '9|9|13', '9|16|0', '9|3', '9|17', '9|0|13', '9|5',\n       '9|12', '9|7|0', '9|16|0|13', '9|2|0', '13|9|13', '16|9|7|0',\n       '9|7|13', '12|9', '9|12|4|3', '9|16|2', '9|4|13', '9|10', '9|2|13',\n       '9|16|3', '9|14', '9|7', '9|12|0', '9|1', '9|5|0', '9|8', '9|2',\n       '9|14|0|13', '9|5|2', '9|5|13', '10|3|0', '10', '10|2', '16|10',\n       '12|10', '10|13', '10|11', '10|2|0', '10|0', '12|10|0', '10|4',\n       '10|3', '16|10|13', '16|10|0', '8|10|13', '12|10|11', '10|5',\n       '13|10', '10|11|0', '10|11|13', '7|10', '10|17', '16|8|10',\n       '16|10|2', '16|11', '11|13', '16|11|0', '12|11', '11|0', '14|11|0',\n       '11', '11|5', '13|10|11', '12', '13', '14', '15|0', '15|16',\n       '15|8', '15', '15|13|0', '15|0|13', '15|12', '15|5', '15|16|0',\n       '15|7', '15|4', '15|5|0', '15|16|13', '15|2|0', '15|14', '15|3',\n       '15|13', '15|16|7|13', '15|2', '15|12|14|0', '16', '17', '17|0',\n       '17|4', '17|13', '17|17', '17|16', '17|3', '0|13|17', '16|0|17',\n       '16|17', '17|16|13', '17|5|3|0', '0|17', '17|16|0', '3|17',\n       '17|2|0', '17|14|0', '12|0|17', '12|17|16', '16|17|14', '17|5',\n       '2|17', '16|17|13', '17|3|0', '17|0|13', '17|3|0|13', '1|17',\n       '17|7', '17|4|2', '7|17', '5|0|17', '16|4|17', '7|0|17', '8|0|17',\n       '18'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"train=train_df['Label'].apply(lambda x:[max(list(map(int,x.split('|'))))<10])\ntrain.tolist()\nflat_list = [item for sublist in train for item in sublist]\ntrain=train_df.iloc[flat_list]\ntrain.head()","metadata":{"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                               image_id    r_mean    g_mean    b_mean  \\\n0  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.019785  0.007022  0.081189   \n1  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.021645  0.011319  0.059531   \n2  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.026710  0.014573  0.054268   \n3  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.018123  0.009205  0.065854   \n4  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.029577  0.014019  0.037737   \n\n   cell_id Label  size1  size2  \n0        1     0    510    656  \n1        2     0    875    748  \n2        3     0    924    760  \n3        4     0    844    538  \n4        5     0    620   1168  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>r_mean</th>\n      <th>g_mean</th>\n      <th>b_mean</th>\n      <th>cell_id</th>\n      <th>Label</th>\n      <th>size1</th>\n      <th>size2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.019785</td>\n      <td>0.007022</td>\n      <td>0.081189</td>\n      <td>1</td>\n      <td>0</td>\n      <td>510</td>\n      <td>656</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.021645</td>\n      <td>0.011319</td>\n      <td>0.059531</td>\n      <td>2</td>\n      <td>0</td>\n      <td>875</td>\n      <td>748</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.026710</td>\n      <td>0.014573</td>\n      <td>0.054268</td>\n      <td>3</td>\n      <td>0</td>\n      <td>924</td>\n      <td>760</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.018123</td>\n      <td>0.009205</td>\n      <td>0.065854</td>\n      <td>4</td>\n      <td>0</td>\n      <td>844</td>\n      <td>538</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.029577</td>\n      <td>0.014019</td>\n      <td>0.037737</td>\n      <td>5</td>\n      <td>0</td>\n      <td>620</td>\n      <td>1168</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train['ID']=train['image_id']+ '_'+ (train['cell_id']).astype(str) \ntrain.head()","metadata":{"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \"\"\"Entry point for launching an IPython kernel.\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                               image_id    r_mean    g_mean    b_mean  \\\n0  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.019785  0.007022  0.081189   \n1  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.021645  0.011319  0.059531   \n2  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.026710  0.014573  0.054268   \n3  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.018123  0.009205  0.065854   \n4  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.029577  0.014019  0.037737   \n\n   cell_id Label  size1  size2                                      ID  \n0        1     0    510    656  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_1  \n1        2     0    875    748  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_2  \n2        3     0    924    760  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_3  \n3        4     0    844    538  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_4  \n4        5     0    620   1168  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_5  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>r_mean</th>\n      <th>g_mean</th>\n      <th>b_mean</th>\n      <th>cell_id</th>\n      <th>Label</th>\n      <th>size1</th>\n      <th>size2</th>\n      <th>ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.019785</td>\n      <td>0.007022</td>\n      <td>0.081189</td>\n      <td>1</td>\n      <td>0</td>\n      <td>510</td>\n      <td>656</td>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.021645</td>\n      <td>0.011319</td>\n      <td>0.059531</td>\n      <td>2</td>\n      <td>0</td>\n      <td>875</td>\n      <td>748</td>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.026710</td>\n      <td>0.014573</td>\n      <td>0.054268</td>\n      <td>3</td>\n      <td>0</td>\n      <td>924</td>\n      <td>760</td>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.018123</td>\n      <td>0.009205</td>\n      <td>0.065854</td>\n      <td>4</td>\n      <td>0</td>\n      <td>844</td>\n      <td>538</td>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.029577</td>\n      <td>0.014019</td>\n      <td>0.037737</td>\n      <td>5</td>\n      <td>0</td>\n      <td>620</td>\n      <td>1168</td>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"index=train['Label'].apply(lambda x:[len(list(map(int,x.split('|'))))<2])\nindex.tolist()\nflat_list = [item for sublist in index for item in sublist]\ntrain=train.iloc[flat_list]\ntrain.head()","metadata":{"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                               image_id    r_mean    g_mean    b_mean  \\\n0  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.019785  0.007022  0.081189   \n1  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.021645  0.011319  0.059531   \n2  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.026710  0.014573  0.054268   \n3  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.018123  0.009205  0.065854   \n4  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.029577  0.014019  0.037737   \n\n   cell_id Label  size1  size2                                      ID  \n0        1     0    510    656  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_1  \n1        2     0    875    748  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_2  \n2        3     0    924    760  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_3  \n3        4     0    844    538  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_4  \n4        5     0    620   1168  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_5  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>r_mean</th>\n      <th>g_mean</th>\n      <th>b_mean</th>\n      <th>cell_id</th>\n      <th>Label</th>\n      <th>size1</th>\n      <th>size2</th>\n      <th>ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.019785</td>\n      <td>0.007022</td>\n      <td>0.081189</td>\n      <td>1</td>\n      <td>0</td>\n      <td>510</td>\n      <td>656</td>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.021645</td>\n      <td>0.011319</td>\n      <td>0.059531</td>\n      <td>2</td>\n      <td>0</td>\n      <td>875</td>\n      <td>748</td>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.026710</td>\n      <td>0.014573</td>\n      <td>0.054268</td>\n      <td>3</td>\n      <td>0</td>\n      <td>924</td>\n      <td>760</td>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.018123</td>\n      <td>0.009205</td>\n      <td>0.065854</td>\n      <td>4</td>\n      <td>0</td>\n      <td>844</td>\n      <td>538</td>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.029577</td>\n      <td>0.014019</td>\n      <td>0.037737</td>\n      <td>5</td>\n      <td>0</td>\n      <td>620</td>\n      <td>1168</td>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train.Label.unique()","metadata":{"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"df=train\ndf.head()","metadata":{"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                               image_id    r_mean    g_mean    b_mean  \\\n0  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.019785  0.007022  0.081189   \n1  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.021645  0.011319  0.059531   \n2  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.026710  0.014573  0.054268   \n3  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.018123  0.009205  0.065854   \n4  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.029577  0.014019  0.037737   \n\n   cell_id Label  size1  size2                                      ID  \n0        1     0    510    656  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_1  \n1        2     0    875    748  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_2  \n2        3     0    924    760  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_3  \n3        4     0    844    538  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_4  \n4        5     0    620   1168  0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_5  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>r_mean</th>\n      <th>g_mean</th>\n      <th>b_mean</th>\n      <th>cell_id</th>\n      <th>Label</th>\n      <th>size1</th>\n      <th>size2</th>\n      <th>ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.019785</td>\n      <td>0.007022</td>\n      <td>0.081189</td>\n      <td>1</td>\n      <td>0</td>\n      <td>510</td>\n      <td>656</td>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.021645</td>\n      <td>0.011319</td>\n      <td>0.059531</td>\n      <td>2</td>\n      <td>0</td>\n      <td>875</td>\n      <td>748</td>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.026710</td>\n      <td>0.014573</td>\n      <td>0.054268</td>\n      <td>3</td>\n      <td>0</td>\n      <td>924</td>\n      <td>760</td>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.018123</td>\n      <td>0.009205</td>\n      <td>0.065854</td>\n      <td>4</td>\n      <td>0</td>\n      <td>844</td>\n      <td>538</td>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.029577</td>\n      <td>0.014019</td>\n      <td>0.037737</td>\n      <td>5</td>\n      <td>0</td>\n      <td>620</td>\n      <td>1168</td>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom tqdm.autonotebook import tqdm\n\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport pandas as pd\n\nimport time\n\ndef visualize2DSoftmax(X, y, model):\n    \"\"\"Function to visualize the classification boundary of a learned model on a 2-D dataset\n\n    Arguments:\n    X -- a numpy array of shape (2, N), where N is the number of data points. \n    y -- a numpy array of shape (N,), which contains values of either \"0\" or \"1\" for two different classes\n    model -- a PyTorch Module object that represents a classifer to visualize. s\n    \"\"\"\n    x_min = np.min(X[:,0])-0.5\n    x_max = np.max(X[:,0])+0.5\n    y_min = np.min(X[:,1])-0.5\n    y_max = np.max(X[:,1])+0.5\n    xv, yv = np.meshgrid(np.linspace(x_min, x_max, num=20), np.linspace(y_min, y_max, num=20), indexing='ij')\n    xy_v = np.hstack((xv.reshape(-1,1), yv.reshape(-1,1)))\n    with torch.no_grad():\n        preds = model(torch.tensor(xy_v, dtype=torch.float32))\n        preds = F.softmax(preds, dim=1).numpy()\n\n    cs = plt.contourf(xv, yv, preds[:,0].reshape(20,20), levels=np.linspace(0,1,num=20), cmap=plt.cm.RdYlBu)\n    sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, style=y, ax=cs.ax)\n\ndef run_epoch(model, optimizer, data_loader, loss_func, device, results, score_funcs, prefix=\"\", desc=None):\n    \"\"\"\n    model -- the PyTorch model / \"Module\" to run for one epoch\n    optimizer -- the object that will update the weights of the network\n    data_loader -- DataLoader object that returns tuples of (input, label) pairs. \n    loss_func -- the loss function that takes in two arguments, the model outputs and the labels, and returns a score\n    device -- the compute lodation to perform training\n    score_funcs -- a dictionary of scoring functions to use to evalue the performance of the model\n    prefix -- a string to pre-fix to any scores placed into the _results_ dictionary. \n    desc -- a description to use for the progress bar.     \n    \"\"\"\n    running_loss = []\n    y_true = []\n    y_pred = []\n    start = time.time()\n    for inputs, labels in tqdm(data_loader, desc=desc, leave=False):\n        #Move the batch to the device we are using. \n        inputs = moveTo(inputs, device)\n        labels = moveTo(labels, device)\n        # print(\"labels\",labels)\n        \n\n        y_hat = model(inputs) #this just computed f_Θ(x(i))\n        # print(\"output\",y_hat)\n        # Compute loss.\n        loss = loss_func(y_hat, labels)\n\n        if model.training:\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n        #Now we are just grabbing some information we would like to have\n        running_loss.append(loss.item())\n\n        if len(score_funcs) > 0 and isinstance(labels, torch.Tensor):\n            #moving labels & predictions back to CPU for computing / storing predictions\n            labels = labels.detach().cpu().numpy()\n            y_hat = y_hat.detach().cpu().numpy()\n            #add to predictions so far\n            y_true.extend(labels.tolist())\n            y_pred.extend(y_hat.tolist())\n    #end training epoch\n    end = time.time()\n    \n    y_pred = np.asarray(y_pred)\n    if len(y_pred.shape) == 2 and y_pred.shape[1] > 1: #We have a classification problem, convert to labels\n        y_pred = np.argmax(y_pred, axis=1)\n    #Else, we assume we are working on a regression problem\n    \n    results[prefix + \" loss\"].append( np.mean(running_loss) )\n    for name, score_func in score_funcs.items():\n        try:\n            results[prefix + \" \" + name].append( score_func(y_true, y_pred) )\n        except:\n            results[prefix + \" \" + name].append(float(\"NaN\"))\n    return end-start #time spent on epoch\n\ndef train_simple_network(model, loss_func, train_loader, val_loader=None, score_funcs=None, \n                         epochs=50, device=\"cpu\", checkpoint_file=None, lr=0.001):\n    \"\"\"Train simple neural networks\n    \n    Keyword arguments:\n    model -- the PyTorch model / \"Module\" to train\n    loss_func -- the loss function that takes in batch in two arguments, the model outputs and the labels, and returns a score\n    train_loader -- PyTorch DataLoader object that returns tuples of (input, label) pairs. \n    val_loader -- Optional PyTorch DataLoader to evaluate on after every epoch\n    score_funcs -- A dictionary of scoring functions to use to evalue the performance of the model\n    epochs -- the number of training epochs to perform\n    device -- the compute lodation to perform training\n    \n    \"\"\"\n    to_track = [\"epoch\", \"total time\", \"train loss\"]\n    if val_loader is not None:\n        to_track.append(\"val loss\")\n    for eval_score in score_funcs:\n        to_track.append(\"train \" + eval_score )\n        if val_loader is not None:\n            to_track.append(\"val \" + eval_score )\n        \n    total_train_time = 0 #How long have we spent in the training loop? \n    results = {}\n    #Initialize every item with an empty list\n    for item in to_track:\n        results[item] = []\n        \n    #SGD is Stochastic Gradient Decent.\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n    #Place the model on the correct compute resource (CPU or GPU)\n    model.to(device)\n    for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n        model = model.train()#Put our model in training mode\n        \n        total_train_time += run_epoch(model, optimizer, train_loader, loss_func, device, results, score_funcs, prefix=\"train\", desc=\"Training\")\n\n        results[\"total time\"].append( total_train_time )\n        results[\"epoch\"].append( epoch )\n        \n        if val_loader is not None:\n            model = model.eval()\n            with torch.no_grad():\n                run_epoch(model, optimizer, val_loader, loss_func, device, results, score_funcs, prefix=\"val\", desc=\"Testing\")\n                    \n    if checkpoint_file is not None:\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'results' : results\n            }, checkpoint_file)\n\n    return pd.DataFrame.from_dict(results)\n\ndef set_seed(seed):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n\nclass Flatten(nn.Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)\n    \nclass View(nn.Module):\n    def __init__(self, *shape):\n        super(View, self).__init__()\n        self.shape = shape\n    def forward(self, input):\n        return input.view(*self.shape) \n    \nclass LambdaLayer(nn.Module):\n    def __init__(self, lambd):\n        super(LambdaLayer, self).__init__()\n        self.lambd = lambd\n        \n    def forward(self, x):\n        return self.lambd(x)\n    \nclass DebugShape(nn.Module):\n    \"\"\"\n    Module that is useful to help debug your neural network architecture. \n    Insert this module between layers and it will print out the shape of \n    that layer. \n    \"\"\"\n    def forward(self, input):\n        print(input.shape)\n        return input\n    \ndef weight_reset(m):\n    \"\"\"\n    Go through a PyTorch module m and reset all the weights to an initial random state\n    \"\"\"\n    if \"reset_parameters\" in dir(m):\n        m.reset_parameters()\n    return\n\ndef moveTo(obj, device):\n    \"\"\"\n    obj: the python object to move to a device, or to move its contents to a device\n    device: the compute device to move objects to\n    \"\"\"\n    if hasattr(obj, \"to\"):\n        return obj.to(device)\n    elif isinstance(obj, list):\n        return [moveTo(x, device) for x in obj]\n    elif isinstance(obj, tuple):\n        return tuple(moveTo(list(obj), device))\n    elif isinstance(obj, set):\n        return set(moveTo(list(obj), device))\n    elif isinstance(obj, dict):\n        to_ret = dict()\n        for key, value in obj.items():\n            to_ret[moveTo(key, device)] = moveTo(value, device)\n        return to_ret\n    else:\n        return obj\n        \ndef train_network(model, loss_func, train_loader, val_loader=None, score_funcs=None, \n                         epochs=50, device=\"cpu\", checkpoint_file=None, \n                         lr_schedule=None, optimizer=None, disable_tqdm=False\n                        ):\n    \"\"\"Train simple neural networks\n    \n    Keyword arguments:\n    model -- the PyTorch model / \"Module\" to train\n    loss_func -- the loss function that takes in batch in two arguments, the model outputs and the labels, and returns a score\n    train_loader -- PyTorch DataLoader object that returns tuples of (input, label) pairs. \n    val_loader -- Optional PyTorch DataLoader to evaluate on after every epoch\n    score_funcs -- A dictionary of scoring functions to use to evalue the performance of the model\n    epochs -- the number of training epochs to perform\n    device -- the compute lodation to perform training\n    \n    \"\"\"\n    if score_funcs == None:\n        score_funcs = {}#Empty set \n    \n    to_track = [\"epoch\", \"total time\", \"train loss\"]\n    if val_loader is not None:\n        to_track.append(\"val loss\")\n    for eval_score in score_funcs:\n        to_track.append(\"train \" + eval_score )\n        if val_loader is not None:\n            to_track.append(\"val \" + eval_score )\n        \n    total_train_time = 0 #How long have we spent in the training loop? \n    results = {}\n    #Initialize every item with an empty list\n    for item in to_track:\n        results[item] = []\n\n        \n    if optimizer == None:\n        #The AdamW optimizer is a good default optimizer\n        optimizer = torch.optim.AdamW(model.parameters())\n        del_opt = True\n    else:\n        del_opt = False\n\n    #Place the model on the correct compute resource (CPU or GPU)\n    model.to(device)\n    for epoch in tqdm(range(epochs), desc=\"Epoch\", disable=disable_tqdm):\n    \n        model = model.train()#Put our model in training mode\n        running_loss = 0.0\n        \n        y_true = []\n        y_pred = []\n\n        start = time.time()\n        for inputs, labels in tqdm(train_loader, desc=\"Train Batch\", leave=False, disable=disable_tqdm):\n      \n            #Move the batch to the device we are using. \n            inputs = moveTo(inputs, device)\n            labels = moveTo(labels, device)\n            \n            if isinstance(labels, torch.Tensor):\n                batch_size = labels.shape[0]\n            elif isinstance(inputs, torch.Tensor):\n                batch_size = inputs.shape[0]\n            else:\n                batch_size = len(inputs)\n\n            # PyTorch stores gradients in a mutable data structure. So we need to set it to a clean state before we use it. \n            #Otherwise, it will have old information from a previous iteration\n            optimizer.zero_grad()\n\n            y_hat = model(inputs) #this just computed f_Θ(x(i))\n\n            # Compute loss.\n            loss = loss_func(y_hat, labels)\n\n            loss.backward()# ∇_Θ just got computed by this one call!\n\n            #Now we just need to update all the parameters! \n            optimizer.step()# Θ_{k+1} = Θ_k − η * ∇_Θ ℓ(y_hat, y)\n\n            #Now we are just grabbing some information we would like to have\n            running_loss += loss.item() * batch_size\n            \n            if len(score_funcs) > 0 and isinstance(labels, torch.Tensor):\n                #moving labels & predictions back to CPU for computing / storing predictions\n                labels = labels.detach().cpu().numpy()\n                y_hat = y_hat.detach().cpu().numpy()\n                for i in range(batch_size):\n                    y_true.append(labels[i])\n                    y_pred.append(y_hat[i,:])\n        #end training epoch\n        end = time.time()\n        total_train_time += (end-start)\n        \n        results[\"epoch\"].append( epoch )\n        results[\"total time\"].append( total_train_time )\n        results[\"train loss\"].append( running_loss )\n        \n        y_pred = np.asarray(y_pred)\n        \n        if len(y_pred.shape) == 2 and y_pred.shape[1] > 1: #We have a classification problem, convert to labels\n            y_pred = np.argmax(y_pred, axis=1)\n            \n        for name, score_func in score_funcs.items():\n            try:\n                results[\"train \" + name].append( score_func(y_true, y_pred) )\n            except:\n                results[\"train \" + name].append(\"NaN\")\n      \n        if val_loader is None:\n            pass\n        else:#Lets find out validation performance as we go!\n            model = model.eval() #Set the model to \"evaluation\" mode, b/c we don't want to make any updates!\n\n            y_true = []\n            y_pred = []\n            \n            val_running_loss = 0.0\n\n            for inputs, labels in val_loader:\n        \n                #Move the batch to the device we are using. \n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                if isinstance(labels, torch.Tensor):\n                    batch_size = labels.shape[0]\n                elif isinstance(inputs, torch.Tensor):\n                    batch_size = inputs.shape[0]\n                else:\n                    batch_size = len(inputs)\n        \n                y_hat = model(inputs)\n            \n                loss = loss_func(y_hat, labels)\n                \n                #Now we are just grabbing some information we would like to have\n                val_running_loss += loss.item() * batch_size\n\n                if len(score_funcs) > 0 and isinstance(labels, torch.Tensor):\n                    #moving labels & predictions back to CPU for computing / storing predictions\n                    labels = labels.detach().cpu().numpy()\n                    y_hat = y_hat.detach().cpu().numpy()\n                    for i in range(batch_size):\n                        y_true.append(labels[i])\n                        y_pred.append(y_hat[i,:])\n                        \n            results[\"val loss\"].append( running_loss )\n\n            y_pred = np.asarray(y_pred)\n\n            if len(y_pred.shape) == 2 and y_pred.shape[1] > 1: #We have a classification problem, convert to labels\n                y_pred = np.argmax(y_pred, axis=1)\n\n            for name, score_func in score_funcs.items():\n                try:\n                    results[\"val \" + name].append( score_func(y_true, y_pred) )\n                except:\n                    results[\"val \" + name].append( \"NaN\" )\n        \n        #In PyTorch, the convention is to update the learning rate after every epoch\n        if not lr_schedule is None:\n            if isinstance(lr_schedule, torch.optim.lr_scheduler.ReduceLROnPlateau):\n                lr_schedule.step(val_running_loss)\n            else:\n                lr_schedule.step()\n        \n        if checkpoint_file is not None:\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'results' : results\n                }, checkpoint_file)\n    if del_opt:\n        del optimizer\n\n    return pd.DataFrame.from_dict(results)\n\n### RNN utility Classes \n\nclass LastTimeStep(nn.Module):\n    \"\"\"\n    A class for extracting the hidden activations of the last time step following \n    the output of a PyTorch RNN module. \n    \"\"\"\n    def __init__(self, rnn_layers=1, bidirectional=False):\n        super(LastTimeStep, self).__init__()\n        self.rnn_layers = rnn_layers\n        if bidirectional:\n            self.num_driections = 2\n        else:\n            self.num_driections = 1    \n    \n    def forward(self, input):\n        #Result is either a tupe (out, h_t)\n        #or a tyuple (out, (h_t, c_t))\n        rnn_output = input[0]\n\n        last_step = input[1]\n        if(type(last_step) == tuple):\n            last_step = last_step[0]\n        batch_size = last_step.shape[1] #per docs, shape is: '(num_layers * num_directions, batch, hidden_size)'\n        \n        last_step = last_step.view(self.rnn_layers, self.num_driections, batch_size, -1)\n        #We want the last layer's results\n        last_step = last_step[self.rnn_layers-1] \n        #Re order so batch comes first\n        last_step = last_step.permute(1, 0, 2)\n        #Finally, flatten the last two dimensions into one\n        return last_step.reshape(batch_size, -1)\n    \nclass EmbeddingPackable(nn.Module):\n    \"\"\"\n    The embedding layer in PyTorch does not support Packed Sequence objects. \n    This wrapper class will fix that. If a normal input comes in, it will \n    use the regular Embedding layer. Otherwise, it will work on the packed \n    sequence to return a new Packed sequence of the appropriate result. \n    \"\"\"\n    def __init__(self, embd_layer):\n        super(EmbeddingPackable, self).__init__()\n        self.embd_layer = embd_layer \n    \n    def forward(self, input):\n        if type(input) == torch.nn.utils.rnn.PackedSequence:\n            # We need to unpack the input, \n            sequences, lengths = torch.nn.utils.rnn.pad_packed_sequence(input.cpu(), batch_first=True)\n            #Embed it\n            sequences = self.embd_layer(sequences.to(input.data.device))\n            #And pack it into a new sequence\n            return torch.nn.utils.rnn.pack_padded_sequence(sequences, lengths.to(input.data.device), \n                                                           batch_first=True, enforce_sorted=False)\n        else:#apply to normal data\n            return self.embd_layer(input)\n\n\n\n### Attention Mechanism Layers\n\nclass AttentionAvg(nn.Module):\n\n    def __init__(self, attnScore):\n        super(AttentionAvg, self).__init__()\n        self.score = attnScore\n    \n    def forward(self, states, context, mask=None):\n        \"\"\"\n        states: (B, T, D) shape\n        context: (B, D) shape\n        output: (B, D), a weighted av\n        \n        \"\"\"\n        \n        B = states.size(0)\n        T = states.size(1)\n        D = states.size(2)\n        \n        scores = self.score(states, context) #(B, T, 1)\n        \n        if mask is not None:\n            scores[~mask] = float(-10000)\n        weights = F.softmax(scores, dim=1) #(B, T, 1) still, but sum(T) = 1\n        \n        context = (states*weights).sum(dim=1) #(B, T, D) * (B, T, 1) -> (B, D, 1)\n        \n        \n        return context.view(B, D) #Flatten this out to (B, D)\n\n\nclass AdditiveAttentionScore(nn.Module):\n\n    def __init__(self, D):\n        super(AdditiveAttentionScore, self).__init__()\n        self.v = nn.Linear(D, 1)\n        self.w = nn.Linear(2*D, D)\n    \n    def forward(self, states, context):\n        \"\"\"\n        states: (B, T, D) shape\n        context: (B, D) shape\n        output: (B, T, 1), giving a score to each of the T items based on the context D\n        \n        \"\"\"\n        T = states.size(1)\n        #Repeating the values T times \n        context = torch.stack([context for _ in range(T)], dim=1) #(B, D) -> (B, T, D)\n        state_context_combined = torch.cat((states, context), dim=2) #(B, T, D) + (B, T, D)  -> (B, T, 2*D)\n        scores = self.v(torch.tanh(self.w(state_context_combined)))\n        return scores\n\nclass GeneralScore(nn.Module):\n\n    def __init__(self, D):\n        super(GeneralScore, self).__init__()\n        self.w = nn.Bilinear(D, D, 1)\n    \n    def forward(self, states, context):\n        \"\"\"\n        states: (B, T, D) shape\n        context: (B, D) shape\n        output: (B, T, 1), giving a score to each of the T items based on the context D\n        \n        \"\"\"\n        T = states.size(1)\n        D = states.size(2)\n        #Repeating the values T times \n        context = torch.stack([context for _ in range(T)], dim=1) #(B, D) -> (B, T, D)\n        scores = self.w(states, context) #(B, T, D) -> (B, T, 1)\n        return scores\n\nclass DotScore(nn.Module):\n\n    def __init__(self, D):\n        super(DotScore, self).__init__()\n    \n    def forward(self, states, context):\n        \"\"\"\n        states: (B, T, D) shape\n        context: (B, D) shape\n        output: (B, T, 1), giving a score to each of the T items based on the context D\n        \n        \"\"\"\n        T = states.size(1)\n        D = states.size(2)\n        \n        scores = torch.bmm(states,context.unsqueeze(2)) / np.sqrt(D) #(B, T, D) -> (B, T, 1)\n        return scores\n    \ndef getMaskByFill(x, time_dimension=1, fill=0):\n    \"\"\"\n    x: the original input with three or more dimensions, (B, ..., T, ...)\n        which may have unsued items in the tensor. B is the batch size, \n        and T is the time dimension. \n    time_dimension: the axis in the tensor `x` that denotes the time dimension\n    fill: the constant used to denote that an item in the tensor is not in use,\n        and should be masked out (`False` in the mask). \n    \n    return: A boolean tensor of shape (B, T), where `True` indicates the value\n        at that time is good to use, and `False` that it is not. \n    \"\"\"\n    to_sum_over = list(range(1,len(x.shape))) #skip the first dimension 0 because that is the batch dimension\n    \n    if time_dimension in to_sum_over:\n        to_sum_over.remove(time_dimension)\n       \n    with torch.no_grad():\n        #Special case is when shape is (B, D), then it is an embedding layer. We just return the values that are good\n        if len(to_sum_over) == 0:\n            return (x != fill)\n        #(x!=fill) determines locations that might be unused, beause they are \n        #missing the fill value we are looking for to indicate lack of use. \n        #We then count the number of non-fill values over everything in that\n        #time slot (reducing changes the shape to (B, T)). If any one entry \n        #is non equal to this value, the item represent must be in use - \n        #so return a value of true. \n        mask = torch.sum((x != fill), dim=to_sum_over) > 0\n    return mask","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df.Label.unique()","metadata":{"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"\nimport torch\nimport os\nimport os.path\nfrom os import path\nimport cv2\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.autonotebook import tqdm\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nimport time\nimport torchvision \nfrom torchvision import transforms\nfrom PIL import Image \nfrom sklearn.metrics import accuracy_score\n# print(torch.zeros(1).cuda())\n# if torch.cuda.is_available():\n#     print(True)\n# else:\n#     print(False)\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nFLAGS={}\nFLAGS['NUM_CLASSES']=10\nclass imagesDataset(Dataset):\n    def __init__(self, path, df, img_size, Transform):\n        self.path = path\n        self.df = df\n        self.img_ids = df['ID'].values\n        self.labels = df['Label'].values\n        self.img_size = img_size        \n        self.transform = Transform\n        \n    def _get_image(self, ID):\n        img = cv2.imread(self.path + '/' + ID + '.jpg', cv2.IMREAD_UNCHANGED)\n\n        img = cv2.resize(img, (self.img_size, self.img_size))\n        img = np.divide(img, 255)\n        img=torch.tensor(img,dtype=torch.float32)\n        img=img.permute(2, 0, 1)\n        return img          \n        \n    def __len__(self):\n        return len(self.df) \n    \n    def __getitem__(self, index):\n        x = self._get_image(self.img_ids[index])\n        x = self.transform(x)\n        y = self.labels[index]\n        y = y.split('|')\n        y = list(map(int, y))            \n        y = np.eye(FLAGS['NUM_CLASSES'], dtype='float')[y]                                    \n        y = y.sum(axis=0)\n        return x, y\n\n\nTransform = transforms.Compose(\n    [transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\ntrain_split, eval_split = train_test_split(df, test_size=0.2, random_state=42)\ntrain_1 = imagesDataset(train_images_path, train_split, 256, Transform)   \ntrain_loader = torch.utils.data.DataLoader(dataset=train_1,batch_size=64, shuffle=True)\n\n\n\n\n\n# model \nclass CNNmodel(nn.Module):\n\n    def __init__(self):\n        super(CNNmodel, self).__init__()\n        self.conv_img = nn.Sequential(\n\n            nn.Conv2d(3, 64, 3, padding=3//2), \n            nn.Tanh(),\n            nn.Conv2d(64, 64, 3, padding=3//2), \n            nn.Tanh(),\n            nn.BatchNorm2d(64),\n            nn.Conv2d(64, 64, 3, padding=3//2), \n            nn.Tanh(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(64, 2*64, 3, padding=3//2), \n            nn.Tanh(),\n            nn.BatchNorm2d(128),\n            nn.Flatten(), \n            nn.Linear(2097152, 10),\n            \n        )\n       \n    def forward(self, input):\n        \n        output=self.conv_img(input)\n        output=F.softmax(output)\n        output=torch.squeeze(output)\n\n        return output\n","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model = CNNmodel()\nloss = nn.BCEWithLogitsLoss()\ncnn_results = train_simple_network(model, loss,train_loader, val_loader=train_loader,score_funcs={'Accuracy': accuracy_score}, device=device,epochs=10)\n\nsns.lineplot(x='epoch', y='val Accuracy', data=cnn_results, label='CNN')\nprint(cnn_results)\nPATH = \"CNNmodel.pt\"\ntorch.save(model.state_dict(), PATH)","metadata":{"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c902636118fb495d9c6102fc41fdba1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:110: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing:   0%|          | 0/1121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:110: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:110: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing:   0%|          | 0/1121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:110: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:110: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing:   0%|          | 0/1121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:110: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:110: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing:   0%|          | 0/1121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:110: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:110: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing:   0%|          | 0/1121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:110: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:110: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing:   0%|          | 0/1121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:110: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:110: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing:   0%|          | 0/1121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:110: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:110: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing:   0%|          | 0/1121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:110: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:110: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing:   0%|          | 0/1121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:110: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:110: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing:   0%|          | 0/1121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:110: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","output_type":"stream"},{"name":"stdout","text":"   epoch    total time  train loss  val loss  train Accuracy  val Accuracy\n0      0   1491.284416    0.727405  0.723057             NaN           NaN\n1      1   2474.056522    0.723134  0.719769             NaN           NaN\n2      2   3468.614671    0.719985  0.727923             NaN           NaN\n3      3   4507.031702    0.717289  0.716744             NaN           NaN\n4      4   5497.920046    0.714753  0.711946             NaN           NaN\n5      5   6495.745980    0.712304  0.718573             NaN           NaN\n6      6   7547.837082    0.710017  0.709266             NaN           NaN\n7      7   8597.104789    0.707939  0.718065             NaN           NaN\n8      8   9639.762457    0.705950  0.705226             NaN           NaN\n9      9  10689.890546    0.704109  0.702188             NaN           NaN\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"# print(predict_image())\nfull_filename=r\"../input/hpa-cell-tiles-sample-balanced-dataset/cells/0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_1.jpg\"\nimg = cv2.imread(full_filename, cv2.IMREAD_UNCHANGED)\nimg = cv2.resize(img, (256, 256))\nimg = np.divide(img, 255)\nimg=torch.tensor(img,dtype=torch.float32).unsqueeze(1)\nimg=img.permute(1,3,2,0 )\nimg=Transform(img)\nmodel=CNNmodel()\nmodel.load_state_dict(torch.load(PATH))\nmodel.eval()  \nx=model(img)\nx=x.detach().numpy()\nprint(x)","metadata":{"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"[1.6125421e-01 5.6209406e-05 1.7336854e-01 4.0363219e-01 1.6325857e-01\n 7.8783482e-02 8.0417149e-06 1.4892442e-02 4.5759333e-03 1.7033918e-04]\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:110: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_model():  \n    model = torchvision.models.resnet50()\n    model.fc = nn.Linear(2048, 10, bias=True)\n    return model\n\ndef get_model34():  \n    model = torchvision.models.resnet34()\n    model.fc = nn.Sequential(nn.Linear(512, 9, bias=True),\n                             nn.Softmax())\n#     output=torch.squeeze(output)\n    return model\n\nresnet50 = get_model()\nresnet34 = get_model34()","metadata":{"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model = get_model()\nloss = nn.BCEWithLogitsLoss()\ncnn_results = train_simple_network(model, loss,train_loader, val_loader=train_loader,score_funcs={'Accuracy': accuracy_score}, device=device,epochs=2)\n\nsns.lineplot(x='epoch', y='val Accuracy', data=cnn_results, label='CNN')\nprint(cnn_results)\nPATH = \"Resenet50.pt\"\ntorch.save(model.state_dict(), PATH)","metadata":{"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6f2cd5a0e2a44daa61c6d1aedc4550b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Testing:   0%|          | 0/1121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/1121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Testing:   0%|          | 0/1121 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"   epoch   total time  train loss  val loss  train Accuracy  val Accuracy\n0      0  1036.696549    0.332261  0.318422             NaN           NaN\n1      1  2070.836360    0.317148  0.316320             NaN           NaN\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"device","metadata":{"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"# print(predict_image())\nfull_filename=r\"../input/hpa-cell-tiles-sample-balanced-dataset/cells/0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_1.jpg\"\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nimg = cv2.imread(full_filename, cv2.IMREAD_UNCHANGED)\nimg = cv2.resize(img, (256, 256))\nimg = np.divide(img, 255)\nimg=torch.tensor(img,dtype=torch.float32).unsqueeze(1)\nimg=img.permute(1,3,2,0 )\nimg=Transform(img)\nimg.to(device)\n# model=CNNmodel()\n# model.load_state_dict(torch.load(PATH))\nmodel.eval()  \nx=model(img.to(device))\n# x=x.detach().numpy()\nprint(x)\n","metadata":{"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"tensor([[-2.4511, -2.8663, -1.6121, -1.9257, -2.1185, -1.7654, -2.7021, -1.6496,\n         -2.2671, -3.1371]], device='cuda:0', grad_fn=<AddmmBackward>)\n","output_type":"stream"}]},{"cell_type":"code","source":"train.head(20)","metadata":{"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                image_id    r_mean    g_mean    b_mean  \\\n0   0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.019785  0.007022  0.081189   \n1   0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.021645  0.011319  0.059531   \n2   0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.026710  0.014573  0.054268   \n3   0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.018123  0.009205  0.065854   \n4   0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.029577  0.014019  0.037737   \n5   0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.040529  0.008827  0.062398   \n6   0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.061733  0.026504  0.071511   \n7   0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0  0.040644  0.010379  0.047766   \n8   6166673c-bbad-11e8-b2ba-ac1f6b6435d0  0.074895  0.037825  0.112530   \n9   6166673c-bbad-11e8-b2ba-ac1f6b6435d0  0.036115  0.012050  0.106811   \n10  6166673c-bbad-11e8-b2ba-ac1f6b6435d0  0.032621  0.024021  0.069970   \n11  6166673c-bbad-11e8-b2ba-ac1f6b6435d0  0.089533  0.058016  0.103859   \n12  6166673c-bbad-11e8-b2ba-ac1f6b6435d0  0.059662  0.039993  0.090818   \n13  6166673c-bbad-11e8-b2ba-ac1f6b6435d0  0.030374  0.027968  0.084342   \n14  6166673c-bbad-11e8-b2ba-ac1f6b6435d0  0.034496  0.026964  0.099146   \n15  6166673c-bbad-11e8-b2ba-ac1f6b6435d0  0.068021  0.030614  0.060372   \n16  6166673c-bbad-11e8-b2ba-ac1f6b6435d0  0.066678  0.029110  0.073650   \n17  6166673c-bbad-11e8-b2ba-ac1f6b6435d0  0.040242  0.024178  0.081500   \n18  6166673c-bbad-11e8-b2ba-ac1f6b6435d0  0.099820  0.044779  0.090752   \n19  6166673c-bbad-11e8-b2ba-ac1f6b6435d0  0.030644  0.021805  0.062661   \n\n    cell_id Label  size1  size2                                       ID  \n0         1     0    510    656   0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_1  \n1         2     0    875    748   0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_2  \n2         3     0    924    760   0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_3  \n3         4     0    844    538   0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_4  \n4         5     0    620   1168   0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_5  \n5         6     0    472    476   0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_6  \n6         7     0    428    555   0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_7  \n7         8     0    578    640   0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_8  \n8         1     0    270    313   6166673c-bbad-11e8-b2ba-ac1f6b6435d0_1  \n9         2     0    408    208   6166673c-bbad-11e8-b2ba-ac1f6b6435d0_2  \n10        3     0    412    353   6166673c-bbad-11e8-b2ba-ac1f6b6435d0_3  \n11        4     0    490    206   6166673c-bbad-11e8-b2ba-ac1f6b6435d0_4  \n12        5     0    296    305   6166673c-bbad-11e8-b2ba-ac1f6b6435d0_5  \n13        6     0    428    299   6166673c-bbad-11e8-b2ba-ac1f6b6435d0_6  \n14        7     0    440    302   6166673c-bbad-11e8-b2ba-ac1f6b6435d0_7  \n15        8     0    600    250   6166673c-bbad-11e8-b2ba-ac1f6b6435d0_8  \n16        9     0    299    208   6166673c-bbad-11e8-b2ba-ac1f6b6435d0_9  \n17       10     0    544    323  6166673c-bbad-11e8-b2ba-ac1f6b6435d0_10  \n18       11     0    263    180  6166673c-bbad-11e8-b2ba-ac1f6b6435d0_11  \n19       12     0    530    205  6166673c-bbad-11e8-b2ba-ac1f6b6435d0_12  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>r_mean</th>\n      <th>g_mean</th>\n      <th>b_mean</th>\n      <th>cell_id</th>\n      <th>Label</th>\n      <th>size1</th>\n      <th>size2</th>\n      <th>ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.019785</td>\n      <td>0.007022</td>\n      <td>0.081189</td>\n      <td>1</td>\n      <td>0</td>\n      <td>510</td>\n      <td>656</td>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.021645</td>\n      <td>0.011319</td>\n      <td>0.059531</td>\n      <td>2</td>\n      <td>0</td>\n      <td>875</td>\n      <td>748</td>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.026710</td>\n      <td>0.014573</td>\n      <td>0.054268</td>\n      <td>3</td>\n      <td>0</td>\n      <td>924</td>\n      <td>760</td>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.018123</td>\n      <td>0.009205</td>\n      <td>0.065854</td>\n      <td>4</td>\n      <td>0</td>\n      <td>844</td>\n      <td>538</td>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.029577</td>\n      <td>0.014019</td>\n      <td>0.037737</td>\n      <td>5</td>\n      <td>0</td>\n      <td>620</td>\n      <td>1168</td>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_5</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.040529</td>\n      <td>0.008827</td>\n      <td>0.062398</td>\n      <td>6</td>\n      <td>0</td>\n      <td>472</td>\n      <td>476</td>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_6</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.061733</td>\n      <td>0.026504</td>\n      <td>0.071511</td>\n      <td>7</td>\n      <td>0</td>\n      <td>428</td>\n      <td>555</td>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_7</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0</td>\n      <td>0.040644</td>\n      <td>0.010379</td>\n      <td>0.047766</td>\n      <td>8</td>\n      <td>0</td>\n      <td>578</td>\n      <td>640</td>\n      <td>0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_8</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>6166673c-bbad-11e8-b2ba-ac1f6b6435d0</td>\n      <td>0.074895</td>\n      <td>0.037825</td>\n      <td>0.112530</td>\n      <td>1</td>\n      <td>0</td>\n      <td>270</td>\n      <td>313</td>\n      <td>6166673c-bbad-11e8-b2ba-ac1f6b6435d0_1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>6166673c-bbad-11e8-b2ba-ac1f6b6435d0</td>\n      <td>0.036115</td>\n      <td>0.012050</td>\n      <td>0.106811</td>\n      <td>2</td>\n      <td>0</td>\n      <td>408</td>\n      <td>208</td>\n      <td>6166673c-bbad-11e8-b2ba-ac1f6b6435d0_2</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>6166673c-bbad-11e8-b2ba-ac1f6b6435d0</td>\n      <td>0.032621</td>\n      <td>0.024021</td>\n      <td>0.069970</td>\n      <td>3</td>\n      <td>0</td>\n      <td>412</td>\n      <td>353</td>\n      <td>6166673c-bbad-11e8-b2ba-ac1f6b6435d0_3</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>6166673c-bbad-11e8-b2ba-ac1f6b6435d0</td>\n      <td>0.089533</td>\n      <td>0.058016</td>\n      <td>0.103859</td>\n      <td>4</td>\n      <td>0</td>\n      <td>490</td>\n      <td>206</td>\n      <td>6166673c-bbad-11e8-b2ba-ac1f6b6435d0_4</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>6166673c-bbad-11e8-b2ba-ac1f6b6435d0</td>\n      <td>0.059662</td>\n      <td>0.039993</td>\n      <td>0.090818</td>\n      <td>5</td>\n      <td>0</td>\n      <td>296</td>\n      <td>305</td>\n      <td>6166673c-bbad-11e8-b2ba-ac1f6b6435d0_5</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>6166673c-bbad-11e8-b2ba-ac1f6b6435d0</td>\n      <td>0.030374</td>\n      <td>0.027968</td>\n      <td>0.084342</td>\n      <td>6</td>\n      <td>0</td>\n      <td>428</td>\n      <td>299</td>\n      <td>6166673c-bbad-11e8-b2ba-ac1f6b6435d0_6</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>6166673c-bbad-11e8-b2ba-ac1f6b6435d0</td>\n      <td>0.034496</td>\n      <td>0.026964</td>\n      <td>0.099146</td>\n      <td>7</td>\n      <td>0</td>\n      <td>440</td>\n      <td>302</td>\n      <td>6166673c-bbad-11e8-b2ba-ac1f6b6435d0_7</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>6166673c-bbad-11e8-b2ba-ac1f6b6435d0</td>\n      <td>0.068021</td>\n      <td>0.030614</td>\n      <td>0.060372</td>\n      <td>8</td>\n      <td>0</td>\n      <td>600</td>\n      <td>250</td>\n      <td>6166673c-bbad-11e8-b2ba-ac1f6b6435d0_8</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>6166673c-bbad-11e8-b2ba-ac1f6b6435d0</td>\n      <td>0.066678</td>\n      <td>0.029110</td>\n      <td>0.073650</td>\n      <td>9</td>\n      <td>0</td>\n      <td>299</td>\n      <td>208</td>\n      <td>6166673c-bbad-11e8-b2ba-ac1f6b6435d0_9</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>6166673c-bbad-11e8-b2ba-ac1f6b6435d0</td>\n      <td>0.040242</td>\n      <td>0.024178</td>\n      <td>0.081500</td>\n      <td>10</td>\n      <td>0</td>\n      <td>544</td>\n      <td>323</td>\n      <td>6166673c-bbad-11e8-b2ba-ac1f6b6435d0_10</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>6166673c-bbad-11e8-b2ba-ac1f6b6435d0</td>\n      <td>0.099820</td>\n      <td>0.044779</td>\n      <td>0.090752</td>\n      <td>11</td>\n      <td>0</td>\n      <td>263</td>\n      <td>180</td>\n      <td>6166673c-bbad-11e8-b2ba-ac1f6b6435d0_11</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>6166673c-bbad-11e8-b2ba-ac1f6b6435d0</td>\n      <td>0.030644</td>\n      <td>0.021805</td>\n      <td>0.062661</td>\n      <td>12</td>\n      <td>0</td>\n      <td>530</td>\n      <td>205</td>\n      <td>6166673c-bbad-11e8-b2ba-ac1f6b6435d0_12</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# print(predict_image())\nfull_filename=r\"../input/hpa-cell-tiles-sample-balanced-dataset/cells/0e63afe6-bbca-11e8-b2bc-ac1f6b6435d0_1.jpg\"\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nimg = cv2.imread(full_filename, cv2.IMREAD_UNCHANGED)\nimg = cv2.resize(img, (256, 256))\nimg = np.divide(img, 255)\nimg=torch.tensor(img,dtype=torch.float32).unsqueeze(1)\nimg=img.permute(1,3,2,0 )\nimg=Transform(img)\nimg.to(device)\nmodel=get_model()\nmodel.load_state_dict(torch.load('./Resenet50.pt'))\nmodel.eval()  \nx=model(img.to(device))\n# x=x.detach().numpy()\nprint(x)\n","metadata":{"trusted":true},"execution_count":17,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-7829e097b424>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./Resenet50.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Resenet50.pt'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: './Resenet50.pt'","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}